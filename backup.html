<!DOCTYPE HTML> <!-- Urban by TEMPLATED templated.co @templatedco Released for free under the Creative Commons Attribution 3.0 license
(templated.co/license) --> <html> <head> <title>Courtney
Paquette</title> <meta charset="utf-8" /> <meta name="viewport"
content="width=device-width, initial-scale=1" /> <link
rel="stylesheet" href="assets/css/main.css" /> </head> <body>


		<!-- Banner --> <section id="banner"> <div
			class="inner"> <header> <h1>Courtney
			Paquette</h1> <p> Assistant Professor,
			Mathematics and Statistics, McGill
			University</p> </header> <a href="#aboutme"
			class="button big scrolly">About Me</a> &nbsp;
			&nbsp; &nbsp; &nbsp; &nbsp; <a
			href="#research" class="button big
			scrolly">Research</a> &nbsp; &nbsp; &nbsp;
			&nbsp; &nbsp; <a href="#presentations"
			class="button big scrolly">Presentations</a> &nbsp; &nbsp; &nbsp;
			&nbsp; &nbsp; <a href="#workshops"
			class="button big scrolly">Workshops &
			Tutorials</a> <p> </p> <a href="#teaching"
			class="button big scrolly">Teaching</a>  &nbsp;
			&nbsp; &nbsp; &nbsp; &nbsp; <a
			href="Research/CV.pdf" class="button big
			scrolly">CV</a> &nbsp; &nbsp; &nbsp;
			&nbsp; &nbsp; <a href="#bio"
			class="button big scrolly">Biosketch</a>&nbsp;
			&nbsp; &nbsp; &nbsp; &nbsp; <a
			href="#RMT_OPT_ML_seminar" class="button big
			scrolly">RMT+ML+OPT seminar</a></div> </section>

		<!-- Main --> <div id="main">

				<!-- Section --> <section
					class="wrapper style1"
					id="aboutme"> <div
					class="inner"> <!-- 2 Columns
					--> <div class="flex flex-2">
					<div class="col col1"> <div
					class="image fit"> <img
					src="images/paquette_photo_1.jpg"
					alt ="" /></a> </div> </div>
					<div class="col col2">
					<h3>HELLO!</h3> <p>I am an
					assistant professor at McGill
					University in the <a
					href=https://www.mcgill.ca/mathstat/><u>Mathematics
					and Statistics department</u>
					</a>. I am a CIFAR AI Chair
					and I am an active member of
					the Montreal Machine Learning
					Optimization Group <a
					href=https://mtl-mlopt.github.io/><u>(MTL
					MLOpt)</u></a> at <a
					href=https://mila.quebec/en/><u>MILA.</u></a>
					Moreover I am the lead
					organizer of the <a
					href=https://opt-ml.org/><u>OPT-ML
					Workshop</u></a> for NeurIPS
					2020. Previously, I was a
					research scientist at <a
					href=https://ai.google/research/teams/brain/><u>Google
					Brain, Montreal.</u></a> You
					can view my CV <a
					href="Research/CV.pdf"><u>here</u></a>
					if you are interested in more
					details.  </p> <p>I received
					my Ph.D. from the Mathematics
					department at the University
					of Washington (2017) under <a
					href=http://www.math.washington.edu/~ddrusv/><u>Prof. Dmitriy
					Drusvyatskiy</u></a> then I
					held a postdoctoral position
					in the Industrial and Systems
					Engineering at Lehigh
					University where I worked with
					<a
					href=https://coral.ise.lehigh.edu/katyas/><u>Prof. Katya
					Scheinberg</u></a>. I held an
					NSF postdoctoral fellowship
					(2018-2019) under <a
					href=https://uwaterloo.ca/scholar/vavasis><u>Prof. Stephen
					Vavasis</u></a> in the
					Combinatorics and Optimization
					Department at the University
					of Waterloo.</p>
									  
									  <!--Currently,
					I am Ross Assistant Professor
					(Post Doctoral position) at
					Ohio State
					University. Starting in
					January 2018, I will be a post
					doc in the Industrial and
					Systems Engineering Department
					of Lehigh University. I will
					be co-advised by <a
					href=https://coral.ise.lehigh.edu/katyas/><u>Prof. Katya
					Scheinberg </u></a>, <a
					href=https://coral.ise.lehigh.edu/frankecurtis/><u>Prof. Frank
					E.  Curtis </u></a>, and <a
					href=http://mtakac.com/><u>Prof. Martin
					Tak&aacute&#x10d
					</u></a>. </p> -->

									  
					<p> My research broadly
					focuses on designing and
					analyzing algorithms for
					large-scale optimization
					problems, motivated by
					applications in data
					science. The techniques I use
					draw from a variety of fields
					including probability,
					complexity theory, and convex
					and nonsmooth analysis. </p>
					<!--I study optimization, in
					particular continuous
					optimization and first order
					methods. My Ph.D advisor at
					the University of Washington
					was <a
					href=http://www.math.washington.edu/~ddrusv/><u>Prof. Dmitriy
Drusvyatskiy</u></a>.-->

<p> For a magazine article about myself and my research, see <a
								  href=https://cifar.ca/publications-reports/reach/><u>REACH
								  Magazine
								  Rising
								  Star
								  in
								  AI, 2022</u></a>
</p>

									 <p>
					University of Washington,
					Lehigh University, University
					of Waterloo, McGill
					University, and MIlA have
					strong optimization groups
					which spans across many
					departments: Math, Stats, CSE,
					EE, and ISE. If you are
					interested in optimization
					talks at these places, check
					out the following seminars:
					<ul><li> <a
					href=https://opt-ml.org/><u>
					Optimization for Machine
					Learning (OPT+ML) </u></a>
					workshop at NeurIPS </li> <li>
					<a
					href=https://mtl-mlopt.github.io/><u>
					Montreal Machine Learning and
					Optimization (MTL MLOPT)
					</u></a> at MILA </li> <li> <a
					href=https://dms.umontreal.ca/~mathapp/><u>
					Applied Mathematics </u></a>
					at McGill University </li>
					<li> <a
					href=http://blogs.uw.edu/tops/><u>Trends
					in Optimization Seminar
					(TOPS/CORE) </u></a> at
					University of Washington </li>
					<li> <a
					href=http://ads-institute.uw.edu/IFDS/about.html><u>Institute
					for Foundations of Data
					Science</u></a> at University
					of Washington/University of
					Wisconsin </li> <li> <a
					href=http://ads-institute.uw.edu/IFDS/about.html><u>Machine
					Learning</u></a> at Paul
					G. Allen School of Computer
					Science and Engineering,
					University of Washington </li>
					<li> <a
					href=https://coral.ise.lehigh.edu/><u>
					COR&#64L </u></a> at Lehigh
					University</li> <li> <a
					href=https://uwaterloo.ca/combinatorics-and-optimization/><u>
					Combinatorics and Optimization
									    
									    </u></a>
					at University of Waterloo <ul>
					<li> <a
					href="Research/U_Waterloo_Seminar_ML.pdf"><u>Machine
					Learning Notes from Fall Term
					2018</u></a></li> <li> <a
					href="Research/U_Waterloo_Seminar_HDP.pdf"><u>High
					Dimensional Probability Notes
					from Spring Term 2019</u></a>
					</li>

									    </li>
									    </ul>
									    </ul>
									    </p>
									    <!--<p>
									    Exciting
									    news!
									    I
									    recently
									    got
									    married
									    to
									    another
									    mathematician,
									    <a
									    href="https://people.math.osu.edu/paquette.30/">
									    <u>Elliot
									    Paquette.</u></a></p>-->
									    <p>
									    EMAIL:
									    yumiko88(at)uw(dot)edu
									    or
									    yumiko88(at)u(dot)washington(dot)edu
									    or
									    courtney(dot)paquette(at)mcgill(dot)ca
									    </p>
									    <p>
									    OFFICE:
									    BURN
									    913
									    </p>
									    </div>
									    </div>
									    </div>
									    </section>

				<!-- Section --> <section
					class="wrapper style2"
					id="research"> <div
					class="inner" > <div
					class="flex flex-2"> <div
					class="col col2">
					<h2>RESEARCH</h2> <p>My
					research interests lie at the
					frontier of large-scale
					continuous optimization.
					Nonconvexity, nonsmooth
					analysis, complexity bounds,
					and interactions with random
					matrix theory and
					high-dimensional statistics
					appear throughout work. Modern
					applications of machine
					learning demand these advanced
					tools and motivate me to
					develop theoretical guarantees
					with an eye towards immediate
					practical value. My current
					research program is concerned
					with developing a coherent
					mathematical framework for
					analyzing average-case
					(typical) complexity and exact
					dynamics of learning
					algorithms in the
					high-dimensional setting.
					</p> <p>You can view my CV <a
					href="Research/CV.pdf"><u>here</u></a>
					if you are interested in more
					details.  </p> <p> You can
					view my thesis titled: <a
					href="Research/CP_thesis_v2.pdf"><u>Structure
					and complexity in non-convex
					and nonsmooth
					optimization. </u></a> </p>
					<h3>PAPERS </h3> <p> * student
					author <ul> <li>C. Paquette,
					E. Paquette, B. Adlam,
					J. Pennington <b> <i
					style="color:#D3D3D3;">
					Implicit Regularization or
					Implicit Conditioning? Exact
					Risk Trajectories of SGD in
					High Dimensions. </i></b>
					(submitted), 2022, <a
					href=><u> arXiv
					pdf</u></a></li>
<p> </p> <li>K. Lee*, A.N. Cheng*, E.Paquette,
    C. Paquette.<b> <i style="color:#D3D3D3;"> Trajectory of
    Mini-Batch Momentum: Batch Size Saturation and Convergence in
    High-Dimensions. </i></b> (submitted), 2022, <a
    href=https://arxiv.org/pdf/2206.01029.pdf><u> arXiv pdf</u></a>
    
    </li> <p> </p> <li>C. Paquette, E. Paquette, B. Adlam,
    J. Pennington <b> <i style="color:#D3D3D3;"> Homogenization of SGD
    in high-dimensions: Exact dynamics and generalization
    properties. </i></b> (submitted), 2022, <a
    href=https://arxiv.org/pdf/2205.07069.pdf><u> arXiv pdf</u></a>
    
    </li> <p> </p> <li>L. Cunha*, G. Gidal, F. Pedregosa, C. Paquette,
    D.Scieur. <b> <i style="color:#D3D3D3;"> Only Tails Matter:
    Average-case Universality and Robustness in the Convex
    Regime. </i></b> accepted ICML, 2022, <a href=><u> pdf</u></a>
    
    </li> <p> </p>

<li>C. Paquette and E. Paquette. <b> <i style="color:#D3D3D3;">
    Dynamics of Stochastic Momentum Methods on Large-scale, Quadratic
    Models. </i></b> Advances in Neural Information Processing Systems
    (NeurIPS), volume 34, 2021, <a
    href=https://proceedings.neurips.cc/paper/2021/file/4cf0ed8641cfcbbf46784e620a0316fb-Paper.pdf><u>
    pdf</u></a>
    
    </li> <p> </p> <li>C. Paquette, K. Lee*, F. Pedregosa, and
    E. Paquette. <b> <i style="color:#D3D3D3;"> SGD in the Large:
    Average-case Analysis, Asymptotics, and Stepsize Criticality.</i>
    </b> Proceedings of Thirty Fourth Conference on Learning Theory
    (COLT) (2021) no. 134, 3548--3626, <a
    http://proceedings.mlr.press/v134/paquette21a/paquette21a.pdf><u>
    pdf</u></a>
								  
								  </li>
								  <p>
								  </p>
								  
<li>C. Paquette, B. van Merrienboer, F. Pedregosa, and E. Paquette.
  <b> <i style="color:#D3D3D3;"> Halting time is predictable for large
  models: A Universality Property and Average-case Analysis.</i> </b>
  (2020) (to appear in <i>Found. Comput. Math.</i>), <a
  href=https://arxiv.org/abs/2006.04299><u>arXiv pdf</u></a>
								    
								  </li>
								  <p>
								  </p>
								  <li>S. Baghal,
								  C. Paquette,
								  and
								  SA
								  Vavasis.
								  <b>
								  <i
								  style="color:#D3D3D3;">A
								  termination
								  criterion
								  for
								  stochastic
								  gradient
								  for
								  binary
								  classification.</i>
								  </b>
								  (2020)
								  (submitted),
								  <a
								  href=https://arxiv.org/abs/2003.10312><u>arXiv
								  pdf</u></a>
								    
								  </li>
								    <p>
								    </p>
								    <li>
								    C. Paquette
								    and
								    S. Vavasis. <b>
								    <i
								    style="color:#D3D3D3;">
								    Potential-based
								    analyses
								    of
								    first-order
								    methods
								    for
								    constrained
								    and
								    composite
								    optimization.
								    </i>
								    </b>
								    (2019)
								    (submitted),
								    <a
								    href=https://arxiv.org/abs/1903.08497><u>
								    arXiv
								    pdf
								    </u>
								    </a>
								 
								    </li>
								    <p>
								    </p>
								    <li>
								    C. Paquette
								    and
								    K. Scheinberg. <b>
								    <i
								    style="color:#D3D3D3;">A
								    stochastic
								    line-search
								    method
								    with
								    convergence
								    rate.</i>
								    </b>
								    SIAM
								    J. Optim. (30)
								    (2020)
								    no. 1,
								    349-376,
								    <a
								    href=https://doi.org/10.1137/18M1216250>
								    doi:10.1137/18M1216250,</u></a>
								    <a
								    href=https://arxiv.org/pdf/1807.07994><u>arXiv
								    pdf</u>
								    </a>
								    </li>
								    <p>
								    </p>
								    <li>
								    D. Davis,
								    D. Drusvyatskiy,
								    K. MacPhee,
								    and
								    C. Paquette.
								    <b>
								    <i
								    style="color:#D3D3D3;">
								    Subgradient
								    methods
								    for
								    sharp
								    weakly
								    convex
								    functions. </i>
								    </b>
								    J. Optim. Theory
								    Appl. (179)
								    (2018)
								    no. 3,
								    962-982,
								    <a
								    href
								    =
								    https://doi.org/10.1007/s10957-018-1372-8><u>doi:10.1007/s10957-018-1372-8</u></a>,
								    <a
								    href=https://arxiv.org/abs/1803.02461><u>
								    arXiv
								    pdf
								    </u>
								    </a>
								    </li>
								    <p>
								    </p>
								    <li>
								    D. Davis,
								    D. Drusvyatskiy,
								    and
								    C. Paquette. <b>
								    <i
								    style="color:#D3D3D3;">
								    The
								    nonsmooth
								    landscape
								    of
								    phase
								    retrieval. </i>
								    </b>
								    IMA
								    J. Numer. Anal. (40)
								    (2020)
								    no.4,
								    2652-2695,
								    <a
								    href=https://doi.org/10.1093/imanum/drz031><u>
								    doi:10.1093/imanum/drz031</u></a>,
								    <a
								    href=https://arxiv.org/abs/1711.03247><u>
								    arXiv
								    pdf
								    </u>
								    </a>
								    </li>
								    <p>
								    </p>
								    <li>
								    C. Paquette,
								    H. Lin,
								    D. Drusvyatskiy,
								    J. Mairal,
								    and
								    Z. Harchaoui. <b><i
								    style="color:#D3D3D3;">Acceleration
								    for
								    Gradient-Based
								    Non-Convex
								    Optimization.
								    </i>
								    </b>
								    22nd
								    International
								    Conference
								    on
								    Artificial
								    Intelligence
								    and
								    Statistics
								    (AISTATS
								    2018),
								    <a
								    href=http://proceedings.mlr.press/v84/paquette18a.html><u>
								    arXiv
								    pdf
								    </a>
								    </u>
								    </li>
								    <p>
								    </p>
								    <li>
								    D. Drusvyatskiy
								    and
								    C. Paquette. <b><i
								    style="color:#D3D3D3;">
								    Efficiency
								    of
								    minimizing
								    compositions
								    of
								    convex
								    functions
								    and
								    smooth
								    maps.</b></i>
								    Math. Program. 178
								    (2019),
								    no. 1-2,
								    Ser. A,
								    503-558,
								    <a
								    href=https://doi.org/10.1007/s10107-018-1311-3><u>
								    doi:10.1007/s10107-018-1311-3</u></a>,
								    <a
								    href=https://arxiv.org/pdf/1605.00125.pdf><u>arXiv
								    pdf</u></a>
								    </li>
								    <p>
								    </p>
								    <li>
								    D. Drusvyatskiy
								    and
								    C. Paquette.
								    <b><i
								    style="color:#D3D3D3;">Variational
								    analysis
								    of
								    spectral
								    functions
								    simplified.</b></i>
								    J. Convex
								    Anal. 25(1),
								    2018.
								    <a
								    href=https://arxiv.org/pdf/1506.05170.pdf><u>
								    arXiv
								    pdf</u></a></li>
								  

								    </ul>
								    </p>
								    													</div>
								<div
								class="col
								col1
								first">
								  	<center>
								    Practice
								    meets
								    theory;
								    predicting
								    performance
								    of
								    SGD
								    on
								    CIFAR-10
								    data
								</center>
								  <br>
								<div
								class="image
								fit">
								<img
								src="images/CIFAR_5M_Emp_Risk_SGD.jpg"
								alt=""
								/></a>
								</div>
									<div
								class="image
								fit">
								<img
								src="images/concentration_sgd_all.jpg"
								alt=""
								/></a>
								</div>
								<center>
								    Exact
								    dynamics
								    of
								    SGD
								    and
								    concentration effects
								</center>
								    <center>
								    <h3>
								    High-dimensional
								    Analysis
								    of
								    Optimization
								    Algorithms</h3>
								    </center>
								<center>
								    Concentration
								    of
								    halting
								    times 
								</center> <br>
								<div
								class="image
								fit">
								<img
								src="images/gd-ls.jpg"
								alt=""
								/></a>
								</div>
									    <center>
								    <h3>Random
								    Matrix
								    Theory
								    &
								    Machine Learning
								    </h3>
								    </center>
								<center>
								Eigenvalues
								    of
								    covariance
								    matrix
								    of
								    MNIST
								    data
								    set
								    using
								    random features
								</center>
								<br>
								<div
								class="image
								fit">
								<img
								src="images/MNIST_eigenvalues.jpg"
								alt=""
								/></a>
								</div>
								<center>
								    <h3>Stochastic Optimization
								    </h3>
								    </center>
								<center>
								SGD + momentum
								    parameter
								    choices 
								</center>
								<br>
								<div
								class="image
								fit">
								<img
								src="images/heatmap_malthusian_main.jpg"
								alt=""
								/></a>
								</div>
								<center>
								    <h3>Nonsmooth
								    & Nonconvex
								    </h3>
								    </center>
								<center>
								Convex
								    composite
								    (nonsmooth,
								    nonconvex)
								    of
								    robust
								    phase retrieval
								</center>
								<br>
								<div
								class="image
								fit">
								<img
								src="images/graph_true_func.jpg"
								alt=""
								/></a>
								</div>
								</div>
								</div>
								</div>
								</section>

					<!-- Section --> <section
					class="wrapper style1"
					id="presentations"> <div
					class="inner"> <!-- 2 Columns
					--> <div class="flex flex-2">
					<div class="col col1"> <div
					class="image round fit"> <img
					src="images/C_Paquette02.jpg" alt
					="" /></a> </div> </div> <div
					class="col col2">
					  <h2>PRESENTATIONS</h2>
					  <p>I have
								    given
								    talks
								    on
								    the
								    research
								    above
								    at
								    the
								    following
								    conferences.
								    

								    <h3>
								    COLLOQUIUM/PLENARY
								    SPEAKER </h3>
								    <ul>
								      <li>
								    Plenary
								    speaker,
								    <i>GroundedML
								    Workshop</i>,
								    <b
								    style="color:#2E8BC0;">10th
								    International
								    Conference
								    on
								    Learning
								    Representations
								    (ICLR)
								      2022,</b>
								    (virtual event),
								    April
								    2022
								      </li>
								      <p>
								      </p>
								      <li>
								      <i>Courant
								    Institute
								    of
								    Mathematical
								    Sciences
								    Colloquium,</i>
								     <b
								     style="color:#2E8BC0;">New
								    York
								    University
								    (NYU),</b>
								    New
								    York,
								    NY 
								    (virtual event),
								    January
								    2022
								      </li>
								      <p>
								      </p>
								      <li>
								    <i>Mathematics
								    Department
								    Colloquium,</i>
								     <b
								     style="color:#2E8BC0;">
								    University
								    of
								    California-Davis
								    (UC-Davis)</b>,
								    Davis,
								    CA
								    (virtual
								    event),
								    January 2022
								      </li>
								      <p>
								      </p>
								      <li>
								    <i>Operations Research and Financial Engineering Colloquium,</i> <b
								     style="color:#2E8BC0;">
								    Princeton
								    University,</b>
								    Princeton,
								    NJ
								    (virtual
								    event),
								    January
								      2022
								      </li>
								      <p>
								      </p>
								      <li>
								      <i>
								    Computational
								    and
								    Applied
								    Mathematics
								    (CAAM)
								    Colloquium,</i>
								    <b
								     style="color:#2E8BC0;">
								    Rice
								    University,</b>
								      Houston, TX, December 2021
								      </li>
								      <p>
								      </p>
								      <li>
								      Plenary
								    speaker,
								    <i>Beyond
								    first-order
								    methods
								    in
								    machine
								    learning
								    systems
								    Workshop</i>,  <b
								     style="color:#2E8BC0;">
								    International
								    Conference
								    on
								    Machine
								    Learning
								    (ICML),</b>
								    (virtual
								    event),
								    July
								      2021
								      </li>
								      <p>
								      </p>
								      <li>
								    <i>Operations
								    Research
								    Center
								    Seminar,
								    Sloan School of Management,</i> <b
								     style="color:#2E8BC0;">
								    Massachusetts
								    Institute
								    of
								    Technology
								    (MIT),
								    </b>
								    Boston,
								    MA,
								    February 2021
								      </li>
								      <p>
								      </p>
								      </li>
								    <i>Operations Research and Information Engineering (ORIE) Colloquium,</i> <b
								     style="color:#2E8BC0;">
								    Cornell
								    University,
								    </b>
								    Ithaca,
								    NY
								    (virtual
								    event),
								    February
								      2021</li>
								      <p>
								      </p>
								      <li>
								    <i>
								    Tutte Colloquium, Combinatorics and Optimization Department,</i> <b
								     style="color:#2E8BC0;">
								    University
								    of
								    Waterloo,</b>
								    Waterloo,
								    ON
								    (virtual
								    event),
								    June
								      2020
								      </li>
								      <p>
								      </p>
								      <li>
								    <i>
								    Center
								    for
								    Artificial
								    Intelligence
								    Design
								    (CAIDA)
								    (colloquium)
								    </i>, <b
								     style="color:#2E8BC0;">
								    University
								    of
								    British
								      Columbia
								    (UBC),</b>
								      Vancouver,
								    BC
								    (virtual
								    event),
								    June
								      2020
								      </li>
								      <p>
								      </p>
								      <li>
								    <i>Math Colloquium,</i> <b
								     style="color:#2E8BC0;">
								    Ohio
								    State
								      University,</b>
								      Columbus,
								    OH,
								    February
								      2019
								      </li>
								      <p>
								      </p>
								      <li>
								    <i>Applied Math Colloquium,</i> <b
								     style="color:#2E8BC0;">
								    Brown
								      University,</b>
								      Providence,
								    RI,
								    February
								      2019
								      </li>
								      <p>
								      </p>
								      <li>
								    <i>
								    Mathematics and Statistics Colloquium,</i> <b
								     style="color:#2E8BC0;">
								    St. Louis
								    University,</b>
								    St. Louis,
								    MO,
								    November
								      2019,
								      </li>


								    </ul>
								    
								  								    
								    <h3>INVITED
								    TALKS</h3>
								    <ul>
								      <li>
								    <i>Conference on the Mathematical Theory of Deep Neural Networks</i>, <b
								     style="color:#2E8BC0;">
								    DeepMath,
								    </b>
								    UC
								    San
								    Diego,
								    CA,
								    November
								      2022
								      </li>
								      <li><i>Adrian Lewis’ 60th Birthday Conference (contributed talk),</i> <b
								     style="color:#2E8BC0;">
								    University
								    of
								    Washington,</b>
								    Seattle,
								    WA,
								    August
								      2022
								      </li>
								      <li>
								    <i>Stochastic Optimization Session (contributed talk),</i> <b
								     style="color:#2E8BC0;">
								    International
								    Conference
								    on
								    Continuous
								    Optimization
								    (ICCOPT
								    2022),</b>
								    Lehigh
								    University,
								    Bethlehem,
								    PA,
								    July
								      2022
								      </li>
								      <li><i>Conference on random matrix theory and numerical linear algebra (contributed talk),</i> <b
								     style="color:#2E8BC0;">
								    University
								    of
								    Washington,</b>
								    Seattle,
								    WA,
								    June
								      2022
								      </li>
								      <li><i>Dynamics of Learning and Optimization in Brains and Machines,</i> <b
								     style="color:#2E8BC0;">
								    UNIQUE
								    Student
								    Symposium,</b>
								    MILA,
								    Montreal,
								    QC,
								    June
								      2022
								      </li>
								      <li><i>Optimization in Data Science (contributed talk),</i> <b
								     style="color:#2E8BC0;">
								    INFORMS
								    Optimization
								    Society
								    Meeting
								    2022,</b>
								    Greenville,
								    SC,
								    March
								      2022
								      </li>
								      <li>
								    <i>Optimization and ML Workshop (contributed talk),</i> <b
								     style="color:#2E8BC0;">
								    Canadian
								    Mathematical
								    Society
								    (CMS),
								    </b>Montreal,
								    QC,
								    December
								      2021
								      </li>
								      <li>
								    <i>Operations
								    Research
								    /Optimization Seminar</i>, <b
								     style="color:#2E8BC0;">
								    UBC-Okanagan
								    and
								    Simon
								    Fraser
								    University,</b>
								    Burnaby,
								    BC,
								    December
								      2021
								      </li>
								      <li><i>
								    Machine Learning Advances and Applications Seminar,</i> <b
								     style="color:#2E8BC0;">
								    Fields
								    Institute
								    for
								    Research
								    in
								    Mathematical
								    Sciences,</b>
								    Toronto,
								    ON,
								    November
								      2021
								      </li>
								      <li><i>Methods
								    for
								    Large-Scale,
								    Nonlinear Stochastic Optimization Session (contributed talk),</i> <b
								     style="color:#2E8BC0;">
								    SIAM
								    Conference
								    on
								    Optimization,</b>
								    Spokane,
								    WA,
								    July
								      2021
								      <li>
								    <i>MILA
								    TechAide
								    AI
								    Conference
								    (invited
								    talk),</i>
								    Montreal,
								    QC,
								    May
								      2021
								      </li>
								      <li>
								    <i>Minisymposium on Random matrices and numerical linear algebra (contributed talk),</i> <b
								     style="color:#2E8BC0;">
								    SIAM
								    Conference
								    on
								    Applied
								    Linear
								    Algebra,</b>,
								    virtual event
								    May
								      2021
								      </li>
								      <li><i>Numerical
								    Analysis
								    Seminar
								    (invited
								    talk),
								    Applied
								    Mathematics,</i>
								   <b
								     style="color:#2E8BC0;">
								    University
								    of
								    Washington,</b>
								    Seattle,
								    WA,
								    April
								      2021
								      </li>
								      <li><i>Applied Mathematics Seminar (invited talk), Applied Mathematics,</i> <b
								     style="color:#2E8BC0;">
								    McGill
								    University,</b>
								    Montreal,
								    QC,
								    January
								    2021
								      </li>
								      <li><i>Optimization and ML Workshop (contributed talk),</i> <b
								     style="color:#2E8BC0;">
								    Canadian
								    Mathematical
								    Society
								    (CMS), </b>
								    Montreal,
								    QC,
								    December
								      2020
								      </li>
								      <li><i>UW Machine Learning Seminar (invited talk), Paul G. Allen School of Computer Science,</i> <b
								     style="color:#2E8BC0;">
								    University
								    of
								    Washington,</b>
								    Seattle,
								    WA,
								    November
								      2020
								      </li>
								      <li><i>Soup and Science (contributed talk),</i> <b
								     style="color:#2E8BC0;">
								    McGill
								    University,</b>
								    Montreal,
								    QC,
								    September
								    2020
								      </li>
								      <li>
								    <i>Conference on Optimization,</i> <b
								     style="color:#2E8BC0;">
								    Fields
								    Institute
								    for
								    Research
								    in
								    Mathematical
								    Science,</b>
								    Toronto,
								    ON,
								    November
								      2019
								      </li>
								      <li> <i>Applied Math Seminar,</i> <b
								     style="color:#2E8BC0;">
								    McGill
								    University,</b>
								    Montreal,
								    QC,
								    February
								    2019
								      </li>
								      <li>
								    <i>Applied Math and Analysis Seminar,</i> <b
								     style="color:#2E8BC0;">
								    Duke
								    University,</b>
								    Durham,
								    NC,
								    January
								      2019
								      </li>
								      <li>
								    <i>Google
								    Brain
								    Tea
								    Talk,</i> <b
								     style="color:#2E8BC0;">
								    Google, </b>
								    Montreal,
								    QC,
								    January
								      2019
								      </li>
								      <li>
								    <i>Young Researcher Workshop, Operations Research and Information Engineering (ORIE),</i> <b
								     style="color:#2E8BC0;">
								    Cornell
								    University,
								    </b>
								    Ithaca,
								    NY,
								    October
								      2018
								      </li>
								      <li><i>DIMACS/NSF-TRIPODS conference,</i> <b
								     style="color:#2E8BC0;">
								    Lehigh
								    University,</b>
								    Bethlehem,
								    PA,
								    July
								      2018
								      </li>
								      <li><i>Session
								    talk,</i><b
								     style="color:#2E8BC0;">
								    INFORMS
								    annual
								    meeting, </b>
								    Houston,
								    TX,
								    October
								      2017
								      </li>
								      <li>
								    <i>Optimization Seminar,</i> <b
								     style="color:#2E8BC0;">
								    Lehigh
								    University,
								    </b>
								    Bethlehem,
								    PA,
								    September
								      2017
								      </li>
								      <li><i>
								    Session
								    talk,</i>
								    <b
								     style="color:#2E8BC0;">
								    SIAM-optimization,
								    </b>
								    Vancouver,
								    BC,
								    May
								      2017
								      </li>
								      <li>
								    <i>
								    Optimization
								    and
								    Statistical
								    Learning,</i>
								    Les
								    Houches,
								    April
								      2017
								      </li>
								      <li>
								      <i>West Coast Optimization Meeting,</i> <b
								     style="color:#2E8BC0;">
								    University
								    of
								    British
								    Columbia
								    (UBC),</b>
								    Vancouver,
								    BC,
								    September
								    2016 </li>

								  </ul></p>
								  <h3>SUMMER
								  SCHOOLS
								  &
								  TUTORIALS</h3>
								  <p>
								  <ul>
								    <li>
								  <i>Nonconvex and Nonsmooth Optimization Tutorial,</i> <u>East Coast Optimization Meeting,</u>  <b
								     style="color:#2E8BC0;">
								  George
								  Mason
								  University,</b>
								  Fairfax,
								  VA,
								  April
								    2022
								    </li>
								    <li>
								    <i>Average Case Complexity Tutorial,</i> <u>Workshop on Optimization under Uncertainty,</u>  <b
								     style="color:#2E8BC0;">
								  Centre
								  de
								  recherches
								  mathematiques
								  (CRM),</b>
								  Montreal,
								  QC,
								  September
								    2021
								    </li>
								    <li>
								  <i>Stochastic Optimization,</i>  <b
								     style="color:#2E8BC0;">
								  Summer
								  School
								  talk
								  for
								  University
								  of
								  Washington’s
								  ADSI
								  Summer
								  School
								  on
								  Foundations
								  of
								  Data
								  Science,</b>
								  Seattle,
								  WA,
								  August
								    2019
								    </li>
								    </ul>
								  </p>
								  
								    													</div>
								</div>
								</div>
								</section>

									<!-- Section --> <section
					class="wrapper style2"
					id="workshops"> <div
					class="inner" > <div
					class="flex flex-2"> <div
					class="col col2">
					<h2>WORKSHOPS & TUTORIALS</h2>
					  <h3>Workshops</h3>
					  I have had the pleasure to
					organize some wonderful
					optimization workshops. Please
					consider submitting papers to
					  these great organizations.
					  <p>
					  </p>
					  <ul>
					    <li> <i
					style="color:#E9DCC9;"> <u> Optimization for
					Machine Learning Workshop part
					of NeurIPS </u> </i> </li> • Program
					Chair (2020,2021,2022) •
					Annual event in early
					December, late November • <a
					href=https://opt-ml.org/>
					Website: https://opt-ml.org/
					</a> • Accepts papers starting
					in July (see website for
					details) </li> <p> </p>
 <li> <i style="color:#E9DCC9;"> <u> Montreal AI Symposium </u> </i> <br> • Program
					Chair (2021) •
					Annual event in early
					September-October • <a
					href=http://montrealaisymposium.com/>
					Website: http://montrealaisymposium.com/
					</a> • Accepts papers starting
					in June (see website for
					details); Must be connected to
					the greater Montreal area </li>
				      </ul>
				      <h3>Tutorials</h3>
					  I have organized the
					following tutorials based on
					my research. For more
					information, please see the
				      corresponding website. <p> </p>
				      <ul>
					    <li> <i
					style="color:#E9DCC9;"> <u> Random Matrix Theory and Machine Learning Tutorial as part
					of ICML </u> </i> </br> • 2021 • <a
					href=https://random-matrix-learning.github.io/>
					Website:https://random-matrix-learning.github.io/
					</a> • 3 hour introductory
					tutorial on applying random
					matrix theory techniques in
					machine learning </li>
					</ul>
				      
								    													</div>
								<div
								class="col
								col1
								  first">
								  <center>
								  Eigenvalues
								of
								Wishart matrices
								  </center>
								  <br>
								<div
								class="image
								round
								fit">
								<img
								src="images/mp_eigenvalues.jpg"
								alt=""
								/></a>
								</div>
								<div
								class="image
								round
								fit">
								<img
								src="images/average_worst.png"
								alt=""
								/></a>
								</div>
									  <center>
								  Average-case analysis
								</center>
								<br>
								<div
								class="image
								round
								fit">
								<img
								src="images/average_case_sub.png"
								alt=""
								/></a>
								</div>
								</div>
								</div>
								</div>
								</section>
				<!-- Section --> <section
					class="wrapper style1"
					id="teaching"> <div
					class="inner"> <!-- 2 Columns
					--> <div class="flex flex-2">
					<div class="col col1"> <div
					class="image round fit"> <img
					src="images/banner.jpg" alt
					="" /></a> </div> </div> <div
					class="col col2">
					<h2>TEACHING</h2> <h3>Current
					Course </h3> <p> <ul> <li>
					Math 417/517 Linear
								Optimization/Honors
								Linear
								Optimization,
								Fall
								2022, <a
					href="https://mycourses2.mcgill.ca/d2l/home">
					Website </a> </li> </ul> </p>
					<p> <h3>Past Courses</h3> <p>
					I have taught the following
					courses: <ul> <b> McGill
					University, Mathematics and
					Statistics Department </b>
					<li> Math 560 (graduate,
					instructor): Numerical
					Optimization, Winter 2021,
								Winter 2022
					</li><li> Math 315
					(undergraduate, instructor):
					Ordinary Differential
					Equations, Fall 2020, Fall
								2021
					</li>
					<li> Math
					597 (graduate, instructor):
								Topics
								course
								on Convex
					Analysis and Optimization,
								Fall 2021 </li>
					</ul> <ul> <b> Lehigh
					University, Industrial and
					Systems Engineering </b> <li>
					ISE 417 (graduate,
					instructor): Nonlinear
					Optimization, Spring 2018
					</li> </ul> <ul> <b>
					University of Washington,
					Mathematics Department </b>
					<li> Math 125 BC/BD
					(undergraduate, TA): Calculus
					II Quiz Section, Winter 2017
					</li> <li> Math 307 E
					(undergraduate, instructor):
					Intro to Differential
					Equations, Winter 2016 </li>
					<li> Math 124 CC
					(undergraduate, TA): Calculus
					1, Autumn 2015 </li> <li> Math
					307 I (undergraduate,
					instructor): Intro to
					Differential Equations, Spring
					2015</li> <li> Math 125 BA/BC
					(undergraduate, TA): Calculus
					2, Winter 2015 </li> <li> Math
					307 K (undergraduate,
					instructor): Intro to
					Differential Equations, Autumn
					2014 </li> <li> Math 307 L
					(undergraduate, instructor):
					Intro to Differential
					Equations, Spring 2014 </li>
					</ul> </p> </div> </div>
					</div> </section>
						<!-- Section --> <section
					class="wrapper style2"
					id="bio"> <div
					class="inner"> <!-- 2 Columns
					--> <div class="flex flex-2">
					<div class="col col1"> <div
					class="image fit"> <img
					src="images/paquette_photo_1.jpg" alt
					="" /></a> </div> </div> <div
					class="col col2">
					  <h2>Biosketch (for
					  talks)</h2>
					  <p> Courtney Paquette is an
								assistant
								professor
								at
								McGill
								University
								and a
								CIFAR
								Canada
								AI
								chair,
								MILA. Paquette’s
								research
								broadly
								focuses
								on
								designing
								and
								analyzing
								algorithms
								for
								large-scale
								optimization
								problems,
								motivated
								by
								applications
								in
								data
								science. She
								received
								her
								PhD
								from
								the
								mathematics
								department
								at the
								University
								of
								Washington
								(2017),
								held
								postdoctoral
								positions
								at
								Lehigh
								University
								(2017-2018)
								and
								University
								of
								Waterloo
								(NSF
								postdoctoral
								fellowship,
								2018-2019),
								and
								was a
								research
								scientist
								at
								Google
								Research,
								Brain
								Montreal
					  (2019-2020).
					  </p>
					  <p> Research currently
								supported
								by
								CIFAR
								AI
								Chair,
								MILA;
								NSERC
								Discovery
								Grant;
								FRQNT New university researcher’s start-up program
					  </p>
					</div> </div>
					</div> </section>
									<!-- Section --> <section
					class="wrapper style1"
					id="RMT_OPT_ML_seminar"> <div
					class="inner"> <!-- 2 Columns
					--> <div class="flex flex-2">
					<div class="col col1"> <div
					class="image fit"> <img
					src="images/RMT_OPT_ML_Seminar.png" alt
					="" /></a> </div> </div> <div
					class="col col2">
					<h2>Random Matrix Theory
								&
								Machine
								Learning
								<br> & Optimization
								Graduate
								Seminar
								(RMT+ML+OPT
								Seminar)</h2> <h3>Current
					Information, Fall 2022 </h3>
								<p>
								All
								are
								welcome
								to
								attend
								(in
					  person) at McGill University. <br> For
								a
								complete
								schedule,
								see Website
							<ul>
					  <li> Website:
					  <a
					href="https://elliotpaquette.github.io/rmtmloptseminar.html">
					 https://elliotpaquette.github.io/rmtmloptseminar.html</a>
					  </li>
					  <li> WHEN: Wednesdays,
					  2:30-3:30 pm</li>
					  <li> WHERE: BURN 1214</li>
					</ul>
					The goal of the seminar is to give
								graduate
								and undergraduate
								students
								the
								opportunity
								to
								learn
								how to
								present
								technical
								papers
								in
								machine
								learning,
								random
								matrix
								theory,
								and
								optimization. 
					</p> </div> </div>
					</div> </section>

		<!-- Footer --> <footer id="footer"> <div
			class="copyright"> <p>&copy; Untitled. All
			rights reserved. Design: <a
			href="https://templated.co">TEMPLATED</a>. </p>
			</div> </footer>

		<!-- Scripts --> <script
			src="assets/js/jquery.min.js"></script>
			<script
			src="assets/js/jquery.scrolly.min.js"></script>
			<script
			src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body> </html>
